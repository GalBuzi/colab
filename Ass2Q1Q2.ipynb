{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ass2Q1 (2).ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GalBuzi/colab/blob/main/Ass2Q1Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWLabK4pgK5X"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIJ5cYuvmsjQ"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from pandas.plotting import scatter_matrix\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PKpOwAYiFmU"
      },
      "source": [
        "foler_data_path = r'drive/My Drive/DeepLearnCourse/ass2/Protocol/'\r\n",
        "\r\n",
        "subjectID = [101,102,103,104,105,106,107,108,109]\r\n",
        "\r\n",
        "activityIDdict = {0: 'transient',\r\n",
        "              1: 'lying',\r\n",
        "              2: 'sitting',\r\n",
        "              3: 'standing',\r\n",
        "              4: 'walking',\r\n",
        "              5: 'running',\r\n",
        "              6: 'cycling',\r\n",
        "              7: 'Nordic_walking',\r\n",
        "              9: 'watching_TV',\r\n",
        "              10: 'computer_work',\r\n",
        "              11: 'car driving',\r\n",
        "              12: 'ascending_stairs',\r\n",
        "              13: 'descending_stairs',\r\n",
        "              16: 'vacuum_cleaning',\r\n",
        "              17: 'ironing',\r\n",
        "              18: 'folding_laundry',\r\n",
        "              19: 'house_cleaning',\r\n",
        "              20: 'playing_soccer',\r\n",
        "              24: 'rope_jumping' }\r\n",
        "\r\n",
        "activityNameToIdDict = {'transient' : 0,\r\n",
        "              'lying' : 1,\r\n",
        "              'sitting' : 2,\r\n",
        "              'standing' :3,\r\n",
        "              'walking' :4,\r\n",
        "              'running' :5,\r\n",
        "              'cycling':6,\r\n",
        "              'Nordic_walking' :7,\r\n",
        "              'watching_TV':9,\r\n",
        "              'computer_work':10,\r\n",
        "              'car driving':11,\r\n",
        "              'ascending_stairs':12,\r\n",
        "              'descending_stairs':13,\r\n",
        "              'vacuum_cleaning':16,\r\n",
        "              'ironing':17,\r\n",
        "              'folding_laundry':18,\r\n",
        "              'house_cleaning':19,\r\n",
        "              'playing_soccer':20,\r\n",
        "              'rope_jumping':24 }\r\n",
        "\r\n",
        "activity_nums = list(activityIDdict.keys())\r\n",
        "activity_names = list(activityIDdict.values())\r\n",
        "\r\n",
        "cols = [\"timestamp\", \"activityID\",\"heartrate\"]\r\n",
        "\r\n",
        "IMUhand = ['handTemperature', \r\n",
        "           'handAcc16_x', 'handAcc16_y', 'handAcc16_z', \r\n",
        "           'handAcc6_x', 'handAcc6_y', 'handAcc6_z', \r\n",
        "           'handGyro_x', 'handGyro_y', 'handGyro_z', \r\n",
        "           'handMagne_x', 'handMagne_y', 'handMagne_z',\r\n",
        "           'handOrientation1', 'handOrientation2', 'handOrientation3', 'handOrientation4']\r\n",
        "\r\n",
        "\r\n",
        "IMUchest = ['chestTemperature', \r\n",
        "           'chestAcc16_x', 'chestAcc16_y', 'chestAcc16_z', \r\n",
        "           'chestAcc6_x', 'chestAcc6_y', 'chestAcc6_z', \r\n",
        "           'chestGyro_x', 'chestGyro_y', 'chestGyro_z', \r\n",
        "           'chestMagne_x', 'chestMagne_y', 'chestMagne_z',\r\n",
        "           'chestOrientation1', 'chestOrientation2', 'chestOrientation3', 'chestOrientation4']\r\n",
        "\r\n",
        "\r\n",
        "IMUankle = ['ankleTemperature', \r\n",
        "           'ankleAcc16_x', 'ankleAcc16_y', 'ankleAcc16_z', \r\n",
        "           'ankleAcc6_x', 'ankleAcc6_y', 'ankleAcc6_z', \r\n",
        "           'ankleGyro_x', 'ankleGyro_y', 'ankleGyro_z', \r\n",
        "           'ankleMagne_x', 'ankleMagne_y', 'ankleMagne_z',\r\n",
        "           'ankleOrientation1', 'ankleOrientation2', 'ankleOrientation3', 'ankleOrientation4']\r\n",
        "\r\n",
        "columns = cols + IMUhand + IMUchest + IMUankle \r\n",
        "columns_with_subject = cols + IMUhand + IMUchest + IMUankle + ['subject_num']\r\n",
        "\r\n",
        "len(columns) # need to be 54\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSLDYBySrUUb"
      },
      "source": [
        "we need to read all train data into one dataframe to work with\r\n",
        "we will read each subject table and concatenate them to eachother"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "too7UhD-vEbo"
      },
      "source": [
        "# Q1 - Data Exploration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5Ll-fQgvOeu"
      },
      "source": [
        "all_data = pd.DataFrame()\r\n",
        "for i in range (1, 10):\r\n",
        "    procData = pd.read_table(foler_data_path + 'subject10{}.dat'.format(i), header=None, sep='\\s+')\r\n",
        "    procData.columns = columns\r\n",
        "    procData['subject_id'] = 100 + i\r\n",
        "    all_data = all_data.append(procData, ignore_index=True)\r\n",
        "\r\n",
        "all_data.reset_index(drop=True, inplace=True)\r\n",
        "all_data.head()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPWMaNI0wP2H"
      },
      "source": [
        "as we can learn from the data above, the data must go through some cleaning, for example:\r\n",
        "\r\n",
        "removing any activity with '0' because this activity means the subject isn't doing any from the activities that were mentioned in the README file.\r\n",
        "\r\n",
        "it has very noisy data and that affects the output because the heartrate values will not be extracted from the data point set in the actual important activities.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e4R9gGv6lcM"
      },
      "source": [
        "#clean data\r\n",
        "all_data = all_data.drop(all_data[all_data['activityID']==0].index)\r\n",
        "all_data = all_data.interpolate()\r\n",
        "# fill all the NaN values in a coulmn with the mean values of the column\r\n",
        "for colName in all_data.columns:\r\n",
        "    all_data[colName] = all_data[colName].fillna(all_data[colName].mean())\r\n",
        "activity_mean = all_data.groupby(['activityID']).mean().reset_index()\r\n",
        "\r\n",
        "all_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-W02RnaKf8j"
      },
      "source": [
        "print('Size of the data: ', all_data.size)\r\n",
        "print('Shape of the data: ', all_data.shape)\r\n",
        "print('Number of columns in the data: ', len(all_data.columns))\r\n",
        "result_id = all_data.groupby(['subject_id']).mean().reset_index()\r\n",
        "print('Number of uniqe ids in the data: ', len(result_id))\r\n",
        "result_act = all_data.groupby(['activityID']).mean().reset_index()\r\n",
        "print('Numbe of uniqe activitys in the data: ',len(result_act))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4kbrH6O9OzC"
      },
      "source": [
        "def plot_samples(df, column_a, column_b, title, figsize=(10,6)):\r\n",
        "    plt.rcParams.update({'font.size': 16})\r\n",
        "    size = range(len(df))\r\n",
        "    f, ax = plt.subplots(figsize=figsize) \r\n",
        "    plt.bar(size, df[column_a])\r\n",
        "    a = ax.set_xticklabels(df[column_b])\r\n",
        "    b = ax.legend(fontsize = 20)\r\n",
        "    c = ax.set_xticks(np.arange(len(df)))\r\n",
        "    d = ax.set_title(title)\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwxaElWz98J5"
      },
      "source": [
        "#present num of samples per subject\r\n",
        "sampels = all_data.groupby(['subject_id']).count().reset_index()\r\n",
        "sampels_to_subject = pd.DataFrame()\r\n",
        "sampels_to_subject['id'] = sampels['subject_id']\r\n",
        "sampels_to_subject['sampels'] = sampels['timestamp']\r\n",
        "sampels_to_subject = sampels_to_subject.sort_values(by=['sampels'])\r\n",
        "plot_samples(sampels_to_subject,'sampels','id','Number Of Samepls Per Subject')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fazkEz2H_2uH"
      },
      "source": [
        "data is balanced for samples per subject, excepts subject 109 which has very little amount of samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_2946B4_xBY"
      },
      "source": [
        "sampels = all_data.groupby(['activityID']).count().reset_index()\r\n",
        "sampels_to_subject = pd.DataFrame()\r\n",
        "sampels_to_subject['activity'] = [activityIDdict[x] for x in sampels['activityID']]\r\n",
        "sampels_to_subject['sampels'] = sampels['timestamp']\r\n",
        "sampels_to_subject = sampels_to_subject.sort_values(by=['sampels'])\r\n",
        "plot_samples(sampels_to_subject,'sampels','activity','Number Of Samepls Per Activity',figsize=(50,10))\r\n",
        "del sampels_to_subject"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj2RuO1NGOtO"
      },
      "source": [
        "rope jumping has significally less samples - the data isn't totally balanced "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaqRYHWnD5Wl"
      },
      "source": [
        "sampels_heart_rate = pd.DataFrame()\r\n",
        "sampels_heart_rate['id'] = result_id['subject_id']\r\n",
        "sampels_heart_rate['heartrate'] = result_id['heartrate']\r\n",
        "sampels_heart_rate = sampels_heart_rate.sort_values(by=['heartrate'])\r\n",
        "plot_samples(sampels_heart_rate,'heartrate','id','Avg heart Rate Per Subject')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUxsw_29MT5U"
      },
      "source": [
        "sampels_heart_rate = pd.DataFrame()\r\n",
        "sampels_heart_rate['activity'] = [activityIDdict[x] for x in result_act['activityID']]\r\n",
        "sampels_heart_rate['heartrate'] = result_act['heartrate']\r\n",
        "sampels_heart_rate = sampels_heart_rate.sort_values(by=['heartrate'])\r\n",
        "\r\n",
        "plot_samples(sampels_heart_rate,'heartrate','activity','Avg heart Rate Per Activity',figsize=(50,10))\r\n",
        "\r\n",
        "print(sampels_heart_rate)\r\n",
        "\r\n",
        "del sampels_heart_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzwfq_9_NDWF"
      },
      "source": [
        "samepls_tempreture = pd.DataFrame()\r\n",
        "samepls_tempreture['hand'] = result_id['handTemperature']\r\n",
        "samepls_tempreture['chest'] = result_id['chestTemperature']\r\n",
        "samepls_tempreture['ankle'] = result_id['ankleTemperature']\r\n",
        "print(samepls_tempreture)\r\n",
        "\r\n",
        "ax = samepls_tempreture.plot(kind='line', figsize=(20,6), title='Avg Tempatures by Subjects')\r\n",
        "a = ax.set_xticklabels(result_id['subject_id'])\r\n",
        "b = ax.legend(fontsize = 20)\r\n",
        "c = ax.set_xticks(np.arange(len(samepls_tempreture)))\r\n",
        "del samepls_tempreture"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MJgn_szSsyW"
      },
      "source": [
        "samepls_tempreture = pd.DataFrame()\r\n",
        "samepls_tempreture['activity'] = [activityIDdict[x] for x in result_act['activityID']]\r\n",
        "samepls_tempreture['hand'] = result_act['handTemperature']\r\n",
        "samepls_tempreture['chest'] = result_act['chestTemperature']\r\n",
        "samepls_tempreture['ankle'] = result_act['ankleTemperature']\r\n",
        "ax = samepls_tempreture.plot(kind='line', figsize=(50,6), title='Avg Tempatures by Activity')\r\n",
        "a = ax.set_xticklabels(samepls_tempreture['activity'])\r\n",
        "b = ax.legend(fontsize = 20)\r\n",
        "c = ax.set_xticks(np.arange(len(samepls_tempreture)))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjM76yh1XWw8"
      },
      "source": [
        "samepls = pd.DataFrame()\r\n",
        "samepls['hand_x'] = result_id['handAcc16_x']\r\n",
        "samepls['hand_y'] = result_id['handAcc16_y']\r\n",
        "samepls['hand_z'] = result_id['handAcc16_z']\r\n",
        "ax = samepls.plot(kind='line', figsize=(20,6), title='Avg Hand Acceleration Value by Subjects')\r\n",
        "a = ax.set_xticklabels(result_id['subject_id'])\r\n",
        "b = ax.legend(fontsize = 20)\r\n",
        "c = ax.set_xticks(np.arange(len(samepls)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8lQ4wkEYE4y"
      },
      "source": [
        "samepls = pd.DataFrame()\r\n",
        "samepls['chest_x'] = result_id['chestAcc16_x']\r\n",
        "samepls['chest_y'] = result_id['chestAcc16_y']\r\n",
        "samepls['chest_z'] = result_id['chestAcc16_z']\r\n",
        "ax = samepls.plot(kind='line', figsize=(20,6), title='Avg Chest Acceleration Value by Subjects')\r\n",
        "a = ax.set_xticklabels(result_id['subject_id'])\r\n",
        "b = ax.legend(fontsize = 20)\r\n",
        "c = ax.set_xticks(np.arange(len(samepls)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U_OKdjKYl6R"
      },
      "source": [
        "samepls = pd.DataFrame()\r\n",
        "samepls['ankle_x'] = result_id['ankleAcc16_x']\r\n",
        "samepls['ankle_y'] = result_id['ankleAcc16_y']\r\n",
        "samepls['ankle_z'] = result_id['ankleAcc16_z']\r\n",
        "ax = samepls.plot(kind='line', figsize=(20,6), title='Avg Ankle Acceleration Value by Subjects')\r\n",
        "a = ax.set_xticklabels(result_id['subject_id'])\r\n",
        "b = ax.legend(fontsize = 20)\r\n",
        "c = ax.set_xticks(np.arange(len(samepls)))\r\n",
        "del samepls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1UdoHZyNfxK"
      },
      "source": [
        "!pip install heatmapz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVxp9kWTLdwD"
      },
      "source": [
        "from heatmap import heatmap, corrplot\r\n",
        "dropped_activity = all_data.drop(['activityID'], axis = 1)\r\n",
        "\r\n",
        "plt.figure(figsize=(20, 20))\r\n",
        "corrplot(dropped_activity.corr(), size_scale=300);\r\n",
        "del dropped_activity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtgIZKu-8Ehx"
      },
      "source": [
        "# Q2 :\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-sUnX8tVCqv"
      },
      "source": [
        "a. Decide your validation strategy for training your model \r\n",
        "\r\n",
        "we will use for train set subjects 101-105 and 109.\r\n",
        "\r\n",
        "subject 106 will be the validation set.\r\n",
        "\r\n",
        "subjects 107-108 will use for test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ybWKT9T-W9g"
      },
      "source": [
        "def get_train_test(all_data):\r\n",
        "  # create the test data\r\n",
        "  subject107 = all_data[all_data['subject_id'] == 107]\r\n",
        "  subject108 = all_data[all_data['subject_id'] == 108]\r\n",
        "  test = subject107.append(subject108)\r\n",
        "\r\n",
        "  #create validate data\r\n",
        "  valid_train = all_data[all_data['subject_id'] == 106]\r\n",
        "  valid_test = valid_train['activityID']\r\n",
        "  valid_train = valid_train.drop(['activityID'], axis=1)\r\n",
        "\r\n",
        "  # create the train data\r\n",
        "  train = all_data[all_data['subject_id'] != 107]\r\n",
        "  train = all_data[all_data['subject_id'] != 108]\r\n",
        "  train = all_data[all_data['subject_id'] != 106]\r\n",
        "\r\n",
        "\r\n",
        "  # # drop the columns id\r\n",
        "  test = test.drop([\"subject_id\"], axis=1) # 107-108\r\n",
        "  train = train.drop([\"subject_id\"], axis=1) # all the rest\r\n",
        "  valid_train = valid_train.drop(['subject_id'], axis=1)\r\n",
        "\r\n",
        "\r\n",
        "  # split train and test to X and y\r\n",
        "  x_train = train.drop(['activityID'], axis=1)\r\n",
        "  x_test = test.drop(['activityID'], axis=1)\r\n",
        "  y_train = train['activityID']\r\n",
        "  y_test = test['activityID']\r\n",
        "\r\n",
        "  return x_train, x_test, valid_train, valid_test, y_train, y_test\r\n",
        "\r\n",
        "x_train, x_test,valid_train, valid_test, y_train, y_test = get_train_test(all_data)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGXVE481Vfgv"
      },
      "source": [
        "b.\tCreate a naïve baseline solution and calculate train and validation score for that solution. This could be predicting last known value, or prediction of the class distribution for each category.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ger9XqmrU8Aq"
      },
      "source": [
        "we will try to create a baseline by mean of heartrate and mean of hand temp to decide which activity was perfomed\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPGnrKwHqjyQ"
      },
      "source": [
        "b = pd.DataFrame()\r\n",
        "b['activity'] = [activityIDdict[x] for x in result_act['activityID']]\r\n",
        "b['heartrate'] = result_act['heartrate']\r\n",
        "b['handTemp'] = result_act['handTemperature']\r\n",
        "b['act_index'] = [activityNameToIdDict[y] for y in b['activity']]\r\n",
        "b.set_index('act_index', inplace=True)\r\n",
        "b.sort_values(by=['heartrate', 'handTemp'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuR5_y9d8ceM"
      },
      "source": [
        "import random\r\n",
        "\r\n",
        "def choose_random_activity(id1,id2):\r\n",
        "  rand = random.randint(0, 1)\r\n",
        "  if rand == 0:\r\n",
        "    return id1\r\n",
        "\r\n",
        "  return id2\r\n",
        "\r\n",
        "\r\n",
        "def rule_based_baseline(record):\r\n",
        "\r\n",
        "    heart_rate = record['heartrate']\r\n",
        "    hand_temp = record['handTemperature']\r\n",
        "    activity = 0      \r\n",
        "    \r\n",
        "    if heart_rate < 95:\r\n",
        "      # Activities: 1,2,3,17\r\n",
        "      if hand_temp < 33:\r\n",
        "        activity = 1\r\n",
        "      elif hand_temp < 34 and hand_temp >= 33:\r\n",
        "        activity = choose_random_activity(2,3)\r\n",
        "      else:\r\n",
        "        activity = 17\r\n",
        "\r\n",
        "      \r\n",
        "    elif heart_rate < 118:\r\n",
        "      # Activities:  16,4\r\n",
        "      if(hand_temp < 33):\r\n",
        "        activity = 4\r\n",
        "      else:\r\n",
        "        activity = 16      \r\n",
        "\r\n",
        "\r\n",
        "    elif heart_rate < 140:\r\n",
        "      # Activities:  6,7,13,12\r\n",
        "      if hand_temp < 32.3 :\r\n",
        "        activity = choose_random_activity(6,7)\r\n",
        "      else:\r\n",
        "        activity = choose_random_activity(12,13)\r\n",
        "\r\n",
        "      \r\n",
        "\r\n",
        "    elif heart_rate >= 140:\r\n",
        "      # Activities: 5,24\r\n",
        "      if hand_temp < 30.2:\r\n",
        "        activity = 24\r\n",
        "      else:\r\n",
        "        activity = 5\r\n",
        "      \r\n",
        "\r\n",
        "    return activity\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7hIfvfx7Est"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix,accuracy_score,log_loss\r\n",
        "\r\n",
        "def get_preds_from_baseline(df):\r\n",
        "  \r\n",
        "  y_preds = []\r\n",
        "\r\n",
        "  for idx,record in df.iterrows():\r\n",
        "    pred = rule_based_baseline(record)\r\n",
        "    y_preds.append(pred)\r\n",
        "\r\n",
        "  return y_preds\r\n",
        "\r\n",
        "y_preds_train = get_preds_from_baseline(x_train)\r\n",
        "print('train base line results:')\r\n",
        "print(accuracy_score(y_preds_train, y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoS7IO42DPP4"
      },
      "source": [
        "y_preds_valid = get_preds_from_baseline(valid_train)\r\n",
        "print('validation base line results:')\r\n",
        "print(accuracy_score(y_preds_valid, valid_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gIW0LsMEIyZ"
      },
      "source": [
        "y_preds_test = get_preds_from_baseline(x_test)\r\n",
        "print('test base line results:')\r\n",
        "print(accuracy_score(y_preds_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21DyPYbNVmtc"
      },
      "source": [
        "we can see that the baseline isn't that bad so heartrate and handTemp has a significance but that is not enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edSNiuR6eOzK"
      },
      "source": [
        "del x_train ,x_test ,valid_train, valid_test, y_train, y_test\r\n",
        "del y_preds_train, y_preds_valid, y_preds_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3kL-A2XZ1VH"
      },
      "source": [
        "c.\tFit a classical machine learning model to the data and get a better and solid benchmark for the neural network model.\r\n",
        "Think which features will be useful for such a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2mzK8ozUNdN"
      },
      "source": [
        "def get_train_valid_test_all_cols_by_subject(data):\r\n",
        "  # create the test data\r\n",
        "  subject107 = data[data['subject_id'] == 107]\r\n",
        "  subject108 = data[data['subject_id'] == 108]\r\n",
        "  test_df = subject107.append(subject108)\r\n",
        "\r\n",
        "  #create validate data\r\n",
        "  valid_df = data[data['subject_id'] == 106]\r\n",
        "\r\n",
        "  # create the train data\r\n",
        "  train_df = data[data['subject_id'] != 107]\r\n",
        "  train_df = data[data['subject_id'] != 108]\r\n",
        "  train_df = data[data['subject_id'] != 106]\r\n",
        "\r\n",
        "  return train_df, valid_df, test_df\r\n",
        "\r\n",
        "train_df, valid_df, test_df = get_train_valid_test_all_cols_by_subject(all_data)\r\n",
        "train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AqB4vY8WDWX"
      },
      "source": [
        "we chose the most correlated columns in data to be trained and tested"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zZ1m6H7Gtee"
      },
      "source": [
        "\r\n",
        "relevant_cols_by_corr_table = ['timestamp', 'activityID','heartrate','subject_id','handTemperature', \r\n",
        "           'handAcc16_x', 'handAcc16_y', 'handAcc16_z', \r\n",
        "           'handAcc6_x', 'handAcc6_y', 'handAcc6_z',\r\n",
        "           'chestTemperature', \r\n",
        "           'chestAcc16_x', 'chestAcc16_y', 'chestAcc16_z', \r\n",
        "           'chestAcc6_x', 'chestAcc6_y', 'chestAcc6_z',\r\n",
        "           'ankleTemperature', \r\n",
        "           'ankleAcc16_x', 'ankleAcc16_y', 'ankleAcc16_z', \r\n",
        "           'ankleAcc6_x', 'ankleAcc6_y', 'ankleAcc6_z',]\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw_xMZB_bw0M"
      },
      "source": [
        "train_df = train_df[relevant_cols_by_corr_table]\r\n",
        "valid_df = valid_df[relevant_cols_by_corr_table]\r\n",
        "test_df = test_df[relevant_cols_by_corr_table]\r\n",
        "del all_data\r\n",
        "train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZxWrMeCsN4I"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "tqdm.pandas()\r\n",
        "\r\n",
        "def get_timeSeries_window_data(df, window_back, jump_by, label, is_categorical):\r\n",
        "  x = pd.DataFrame(columns=relevant_cols_by_corr_table)\r\n",
        "  y = pd.DataFrame(columns=['activityID'])\r\n",
        "  df_copy = df.copy()\r\n",
        "  subject_list = df_copy.subject_id.unique()\r\n",
        "  activity_list = df_copy.activityID.unique()\r\n",
        "\r\n",
        "  for sub in tqdm(subject_list, desc='subject'):\r\n",
        "    for act in tqdm(activity_list, desc='target'):\r\n",
        "      subject_activity_df = df_copy[(df_copy.subject_id == sub) & (df_copy.activityID == act)]\r\n",
        "      subject_activity_df = subject_activity_df.drop(columns=['subject_id'])\r\n",
        "\r\n",
        "      activity_df = pd.DataFrame(data=subject_activity_df[label], columns=['activityID'])\r\n",
        "      # print(activity_df)\r\n",
        "      subject_activity_df = subject_activity_df.drop(columns=[label])\r\n",
        "\r\n",
        "      for i in range(window_back, subject_activity_df.shape[0], jump_by):\r\n",
        "        # first_row = subject_activity_df.iloc[i-window_back]\r\n",
        "        # last_row = subject_activity_df.iloc[i]\r\n",
        "\r\n",
        "        # first_row_time = first_row['timestamp']\r\n",
        "        # last_row_time = last_row['timestamp']\r\n",
        "        # # print(last_row_time - first_row_time)\r\n",
        "        # if last_row_time - first_row_time != 2:\r\n",
        "        #   continue\r\n",
        "\r\n",
        "        prev_window = subject_activity_df[i-window_back:i]\r\n",
        "        prev_win_mean = prev_window.rolling(window=window_back).mean()\r\n",
        "        x = x.append(prev_win_mean.tail(1),ignore_index=True)\r\n",
        "\r\n",
        "        # print(type(prev_window))\r\n",
        "        # print(prev_window)\r\n",
        "        # print('==================================================================')\r\n",
        "        # print(prev_window.rolling(window=window_back).mean())\r\n",
        "        \r\n",
        "        # print(i)\r\n",
        "        # print(activity_df.iloc[i])\r\n",
        "        # print(activity_df)\r\n",
        "        # print(activity_df.index[0])\r\n",
        "        # index = activity_df.index[0] + 200\r\n",
        "        # print(index)\r\n",
        "        prev_window_act = activity_df[i-window_back:i]\r\n",
        "        # print(prev_window_act.tail(1))\r\n",
        "\r\n",
        "        y = y.append(prev_window_act.tail(1), ignore_index=True )\r\n",
        "\r\n",
        "        # print(x,y)\r\n",
        "      #   break\r\n",
        "      # break\r\n",
        "\r\n",
        "    x = x.drop(columns=['activityID','subject_id'])\r\n",
        "    return x, y\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfVXBcsMM8HB"
      },
      "source": [
        "window_back = 200\r\n",
        "label = 'activityID'\r\n",
        "is_categorical = True\r\n",
        "jump_by = 50\r\n",
        "\r\n",
        "# bla , blabla = get_timeSeries_window_data(train_df, window_back, jump_by, label, True)\r\n",
        "x_train, y_train = get_timeSeries_window_data(train_df, window_back, jump_by, label, True)\r\n",
        "x_val, y_val = get_timeSeries_window_data(valid_df, window_back, jump_by, label, True)\r\n",
        "x_test, y_test = get_timeSeries_window_data(test_df, window_back, jump_by, label, True)\r\n",
        "print(len(x_train))\r\n",
        "print(len(y_train))\r\n",
        "print(len(x_val))\r\n",
        "print(len(y_val))\r\n",
        "print(len(x_test))\r\n",
        "print(len(y_test))\r\n",
        "print('==================================================================')\r\n",
        "\r\n",
        "print(x_train)\r\n",
        "print(y_train)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5V48jnzYQnE"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "\r\n",
        "logReg = LogisticRegression(solver='lbfgs', max_iter=1000)\r\n",
        "ranForest = RandomForestClassifier()\r\n",
        "\r\n",
        "y_train = y_train.astype('int')\r\n",
        "\r\n",
        "logReg.fit(x_train, y_train)\r\n",
        "ranForest.fit(x_train, y_train)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8UZSRbqrq-L"
      },
      "source": [
        "\r\n",
        "logReg_preds_valid = logReg.predict(x_val)\r\n",
        "ranForest_preds_valid = ranForest.predict(x_val)\r\n",
        "\r\n",
        "y_val = y_val.astype('int')\r\n",
        "\r\n",
        "print(f'Random Forest accuracy on validation: {accuracy_score(ranForest_preds_valid, y_val)}')\r\n",
        "print(f'Logistic Regression accuracy on validation: {accuracy_score(logReg_preds_valid, y_val)}')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aQvmkJ1rvpv"
      },
      "source": [
        "\r\n",
        "logReg_preds_test = logReg.predict(x_test)\r\n",
        "ranForest_preds_test = ranForest.predict(x_test)\r\n",
        "\r\n",
        "y_test = y_test.astype('int')\r\n",
        "\r\n",
        "# print(f'SVM accuracy on test: {accuracy_score(dtree_preds_test, y_test)}')\r\n",
        "print(f'Random Forest accuracy on test: {accuracy_score(ranForest_preds_test, y_test)}')\r\n",
        "print(f'Logistic Regression accuracy on test: {accuracy_score(logReg_preds_test, y_test)}')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZzdfEFSWMnc"
      },
      "source": [
        "pretty good results for logistic regression and solid result for randomForest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1YxNth_xfrL"
      },
      "source": [
        "del x_train, x_test, x_val, y_val, y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcAAN2qSHb-u"
      },
      "source": [
        "# d.\tConstruct a neural network model and fit it to the data. analyze the results (Use visualizations to present your loss and other metrics you find relevant, show examples for good and bad classification with high probability, and refer to the uncertain predictions.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6--jp7JfsiP"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\n",
        "from tensorflow.keras import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlDSdHVW5B7L"
      },
      "source": [
        "def confusion_matrix_of_model(model, x, y):\r\n",
        "\r\n",
        "  preds = model.predict(x)\r\n",
        "  pred_np = np.argmax(preds,axis=1)\r\n",
        "  y_np = np.argmax(y, axis=1)\r\n",
        "  print(\"confusion_matrix_predict:\")\r\n",
        "  print('model accuracy on test set is: {0:.2f}%'.format(accuracy_score(y_np,pred_np)*100))\r\n",
        "  sns.heatmap(confusion_matrix(y_np,pred_np),cmap='Blues',annot=False, fmt='d')\r\n",
        "  plt.xlabel('Prediction')\r\n",
        "  plt.ylabel('True label')\r\n",
        "  plt.title(' Confusion matrix')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2vw9z2mfGSq"
      },
      "source": [
        "def plot_results(history):\r\n",
        "    #accuracy\r\n",
        "    plt.plot(history.history['accuracy'])\r\n",
        "    plt.plot(history.history['val_accuracy'])\r\n",
        "    plt.title('model accuracy')\r\n",
        "    plt.ylabel('accuracy')\r\n",
        "    plt.xlabel('epoch')\r\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\r\n",
        "    plt.show()\r\n",
        "    \r\n",
        "    # \"Loss\"\r\n",
        "    plt.plot(history.history['loss'])\r\n",
        "    plt.plot(history.history['val_loss'])\r\n",
        "    plt.title('model loss')\r\n",
        "    plt.ylabel('loss')\r\n",
        "    plt.xlabel('epoch')\r\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3lJgD0YsVa4"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "\r\n",
        "def get_scaled_data(df,label):\r\n",
        "  scaler = MinMaxScaler(feature_range=(0, 1))\r\n",
        "  train_df_scaled = scaler.fit_transform(df)\r\n",
        "  predict_train_scaled = scaler.fit_transform(df[[label]])\r\n",
        "\r\n",
        "  x_t = []\r\n",
        "  y_t = []\r\n",
        "  win = 200\r\n",
        "  for i in range(win,train_df_scaled.shape[0],50):\r\n",
        "    x_t.append(train_df_scaled[i-win:i,:])\r\n",
        "    y_t.append(predict_train_scaled[i])\r\n",
        "  \r\n",
        "  return np.array(x_t) , np.array(y_t)\r\n",
        "\r\n",
        "x_train, y_train = get_scaled_data(train_df,'activityID')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxJeV8twxI-1"
      },
      "source": [
        "x_val, y_val = get_scaled_data(valid_df,'activityID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM-Ujc2QDEZI"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "hot = OneHotEncoder(handle_unknown='ignore', sparse=False)\r\n",
        "hot.fit(y_train)\r\n",
        "hot.fit(y_val)\r\n",
        "\r\n",
        "y_train = hot.transform(y_train)\r\n",
        "y_val = hot.transform(y_val)\r\n",
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7eg3Pq7L1R2"
      },
      "source": [
        "def create_callbacks(path, patience=5, factor=0.1, monitor='val_loss', save_best=True):\r\n",
        "    es = EarlyStopping(monitor=monitor, patience=patience, verbose=1,)\r\n",
        "    rlr = ReduceLROnPlateau(monitor=monitor, factor=factor, patience=patience)\r\n",
        "    model_ckpt = ModelCheckpoint(path, monitor=monitor, save_best_only=save_best)\r\n",
        "    return [es, rlr, model_ckpt]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdgeHZqyuqcm"
      },
      "source": [
        "num = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1twFVYCB7-W"
      },
      "source": [
        "path_to_save_model_activity = r'drive/My Drive/DeepLearnCourse/ass2/model_best_activity'+str(num)+'.h5'\r\n",
        "\r\n",
        "model_activity = Sequential()\r\n",
        "model_activity.add(LSTM(24, input_shape=(x_train.shape[1],x_train.shape[2]), return_sequences=True))\r\n",
        "model_activity.add(Dense(24))\r\n",
        "model_activity.add(LSTM(12, return_sequences=False))\r\n",
        "model_activity.add(Dense(12))\r\n",
        "model_activity.add(Dense(y_train.shape[1],activation='softmax'))\r\n",
        "model_activity.compile( optimizer= 'adam',loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "model_activity.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq-XpRrdfYji"
      },
      "source": [
        "history_act = model_activity.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_val, y_val),\r\n",
        "                      callbacks=create_callbacks(path=path_to_save_model_activity))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKGjLaPdR9rp"
      },
      "source": [
        "plot_results(history_act)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCT4dqKvRTNT"
      },
      "source": [
        "x_test, y_test = get_scaled_data(test_df,'activityID')\r\n",
        "hot.fit(y_test)\r\n",
        "y_test = hot.fit_transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XghXR-PiPAXy"
      },
      "source": [
        "model_activity.load_weights(path_to_save_model_activity)\r\n",
        "model_activity.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO9JX-VrSWuX"
      },
      "source": [
        "model_activity.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSH2gu-1M2DU"
      },
      "source": [
        "predictions = model_activity.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyeTgw-t-7tU"
      },
      "source": [
        "confusion_matrix_of_model(model_activity,x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TSXgLy4WncL"
      },
      "source": [
        "e.\tPretrain your model on the one of the tasks you suggested in 1c and fine tune the trained model to the data. compare the results you got to previous sections (c-d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNkE8Eg9Ws8N"
      },
      "source": [
        "we chose to create a model that make self-supervied mission:\r\n",
        "trying to predict the heart rate based on the other sensors and then used that model to predict the activity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IySRzfstYLKU"
      },
      "source": [
        "x_train_hr, y_train_hr = get_scaled_data(train_df,'heartrate')\r\n",
        "x_val_hr, y_val_hr = get_scaled_data(valid_df,'heartrate')\r\n",
        "x_test_hr, y_test_hr = get_scaled_data(test_df,'heartrate')\r\n",
        "\r\n",
        "x_train_hr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkQ6lCGXZGNr"
      },
      "source": [
        "path_to_save_model_heartrate = r'drive/My Drive/DeepLearnCourse/ass2/model_best_heartrate'+str(num)+'.h5'\r\n",
        "\r\n",
        "model_heartrate = Sequential()\r\n",
        "model_heartrate.add(LSTM(24, input_shape=(x_train_hr.shape[1],x_train_hr.shape[2]), return_sequences=True))\r\n",
        "model_heartrate.add(Dense(12, activation='relu'))\r\n",
        "model_heartrate.add(Dense(y_train_hr.shape[1], activation='relu'))\r\n",
        "\r\n",
        "model_heartrate.compile(optimizer='adam', loss='mse', metrics=['mae', tf.keras.metrics.RootMeanSquaredError()])\r\n",
        "model_heartrate.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGAounVDZ4Hd"
      },
      "source": [
        "history_hr = model_heartrate.fit(x_train_hr, y_train_hr, batch_size=128, epochs=10, validation_data=(x_val_hr, y_val_hr),\r\n",
        "                    callbacks=create_callbacks(path=path_to_save_model_heartrate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP1Oe29obG-o"
      },
      "source": [
        "model_heartrate.evaluate(x_test_hr, y_test_hr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBwyYwu2_LDJ"
      },
      "source": [
        "confusion_matrix_of_model(model_heartrate,x_test_hr,y_test_hr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKtS3X2qdn4_"
      },
      "source": [
        "changing output layer to use weights of heartrate based model to predict activity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Kch86M8d4mU"
      },
      "source": [
        "output = model_heartrate.layers[-2].output\r\n",
        "\r\n",
        "x = LSTM(12, return_sequences=False)(output)\r\n",
        "outputs = Dense(y_train.shape[1], activation='softmax')(x)\r\n",
        "\r\n",
        "for layer in model_heartrate.layers:\r\n",
        "    layer.trainable = False\r\n",
        "\r\n",
        "transfer_model = Model(inputs=model_heartrate.inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yHOsQH3e5lW"
      },
      "source": [
        "transfer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\r\n",
        "transfer_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FL6oKcFe__y"
      },
      "source": [
        "path_to_save_model_transfer = r'drive/My Drive/DeepLearnCourse/ass2/model_best_transfer'+str(num)+'.h5'\r\n",
        "\r\n",
        "transfer_history = transfer_model.fit(x_train_hr, y_train, batch_size=128, epochs=10, validation_data=(x_val_hr, y_val),\r\n",
        "                    callbacks=create_callbacks(path=path_to_save_model_transfer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rlhe76WHrN99"
      },
      "source": [
        "plot_results(transfer_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJs-QwB5sHva"
      },
      "source": [
        "transfer_model.evaluate(x_test_hr, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v7x2Bto0rCo"
      },
      "source": [
        "del x_train_hr, y_train_hr,x_val_hr, y_val_hr,x_test_hr, y_test_hr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS0hW0BUzyvb"
      },
      "source": [
        "f. model improvements ideas:\r\n",
        "\r\n",
        "1. adding activations to the layers\r\n",
        "\r\n",
        "2. Adding more complexity to the model (more LSTM layers)\r\n",
        "\r\n",
        "3. Adding regularization to the model to prevent overfitting\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTQFzOUI0Wwf"
      },
      "source": [
        "g. implementation of improvements:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jpy4ZAX4HvU"
      },
      "source": [
        "improvement 1 - adding activations to the layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h5Df5ll2Z_F"
      },
      "source": [
        "improve1 = Sequential()\r\n",
        "improve1.add(LSTM(25, input_shape=(x_train.shape[1],x_train.shape[2]), return_sequences=True, activation='tanh'))\r\n",
        "improve1.add(Dense(25, activation='relu'))\r\n",
        "improve1.add(LSTM(12, return_sequences=False, activation='tanh'))\r\n",
        "improve1.add(Dense(12, activation='relu'))\r\n",
        "improve1.add(Dense(y_train.shape[1],activation='softmax'))\r\n",
        "improve1.compile( optimizer= 'adam',loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "improve1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7CoHv9a4vB6"
      },
      "source": [
        "path_to_save_model_imp1 = r'drive/My Drive/DeepLearnCourse/ass2/model_best_imp1'+str(num)+'.h5'\r\n",
        "\r\n",
        "imp1_history = improve1.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_val, y_val),\r\n",
        "                    callbacks=create_callbacks(path=path_to_save_model_imp1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx5wqbSx5_PF"
      },
      "source": [
        "plot_results(imp1_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W16UIW3y686l"
      },
      "source": [
        "confusion_matrix_of_model(improve1,x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ersQua08Wv4"
      },
      "source": [
        "improvement 2 - adding complexity to the layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwTuN-1d8fmX"
      },
      "source": [
        "improve2 = Sequential()\r\n",
        "improve2.add(LSTM(64, input_shape=(x_train.shape[1],x_train.shape[2]), return_sequences=True, activation='tanh'))\r\n",
        "improve2.add(Dense(64, activation='relu'))\r\n",
        "improve2.add(LSTM(32, return_sequences=False, activation='tanh'))\r\n",
        "improve2.add(Dense(32, activation='relu'))\r\n",
        "improve2.add(Dense(y_train.shape[1],activation='softmax'))\r\n",
        "improve2.compile( optimizer= 'adam',loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "improve2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh_xEcw28qX-"
      },
      "source": [
        "path_to_save_model_imp2 = r'drive/My Drive/DeepLearnCourse/ass2/model_best_imp2'+str(num)+'.h5'\r\n",
        "\r\n",
        "imp2_history = improve2.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_val, y_val),\r\n",
        "                    callbacks=create_callbacks(path=path_to_save_model_imp2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mlg_dD68_9C"
      },
      "source": [
        "plot_results(imp2_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcodC0Vr9IRl"
      },
      "source": [
        "confusion_matrix_of_model(improve2,x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}